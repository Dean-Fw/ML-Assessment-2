{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Assessment 2\n",
    "\n",
    "Through the following notebook, you will be tasked with **implementing** two different classification approaches: **k Nearest Neighbours** and **Decision Trees**. You will apply these to two different datasets, evaluate the produced models and analyse their performance.\n",
    "\n",
    "This will be done through several different tasks and sub-tasks:\n",
    "- [Dataset](#Dataset)\n",
    "    - [Task 1: Dataset statistics **(10\\%)**](#Task-1---Dataset-statistics)\n",
    "- [k Nearest Neigbours](#k-Nearest-Neigbours)\n",
    "    - [Task 2.1: Euclidean distance **(10\\%)**](#Task-2.1---Euclidean-distance)\n",
    "    - [Task 2.2: kNN classifier **(20\\%)**](#Task-2.2---kNN-classifier)\n",
    "- [Decision Trees](#Decision-Trees)\n",
    "    - [Task 3.1: Entropy and Information Gain **(10\\%)**](#Task-3.1---Entropy-and-Information-Gain)\n",
    "    - [Task 3.2: Decision Tree classifier **(30\\%)**](#Task-3.2---Decision-Tree-classifier)\n",
    "- [Model evaluation and analysis](#Model-evaluation-and-analysis)\n",
    "    - [Task 4.1: Evaluation **(10\\%)**](#Task-4.1---Evaluation)\n",
    "    - [Task 4.2: Analysis **(10\\%)**](#Task-4.2---Analysis)\n",
    "\n",
    "**Note:** The (%) noted above are out of 100; this will be scaled down to **maximum of 70 marks** for the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For this assessment, you will be working with two datasets:\n",
    "- A [numeric-only](#Numerical-data) dataset (crystal systems of Li-ion batteries)\n",
    "- A dataset with [mixed numeric and categorical](#Categorical-data) features (forest cover)\n",
    "\n",
    "For developing your solutions, **you have only been provided with a portion of the samples from each of the dataset** (70% of the [numeric-only](#Numerical-data) and 50% of the [mixed numeric and categorical](#Categorical-data) data). These will be in **identical format** to the full datasets used for evaluating your solutions (there will just be more samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical data\n",
    "\n",
    "The main part of the assessment will be done on the dataset containing only numerical features describing the physical and chemical properties of the Li-ion battery, which can be classified on the basis of their crystal system. Three major classes of crystal systems are: _monoclinic_, _orthorhombic_ and _triclinic_. (The dataset for this assessment has been adapted from the full dataset which can be found [here](https://www.kaggle.com/datasets/divyansh22/crystal-system-properties-for-liion-batteries\n",
    "\n",
    "Each sample corresponds to the properties of a battery, and consists of following features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Formation Energy`       | `float`: eV | Formation energy of the material. |\n",
    "| `E Above Hull` | `float`: eV | Energy of decomposition of material into most stable ones. |\n",
    "| `Band Gap` | `float`: eV | Band gap. |\n",
    "| `Nsites` | `int`: count | Number of atoms in the unit cell of the crystal. |\n",
    "| `Density` | `float`: gm/cc | The density of bulk crystalline materials. |\n",
    "| `Volume` | `float` | The unit cell volume of the material. |\n",
    "\n",
    "The goal for the assessment is to predict whether the crystal system of the battery is _monoclinic_, _orthorhombic_ or _triclinic_, which provides a classification for each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Crystal System`  | `string`: class designation | Class of the crystal system (three different values). |\n",
    "\n",
    "\n",
    "This dataset will be used for the developmet and testing of both the [kNN](#k-Nearest-Neigbours) and [Decision Tree](#Decision-Trees) classifier, as well as for the [evaluation](#Task-4.1---Evaluation) of both of the classification approaches.\n",
    "\n",
    "The following cell loads the dataset, and splits the samples randomly, using  $80\\%$ of samples for training and $20\\%$ for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 237 samples\n",
      "The numerical data loaded consists of following columns:\n",
      "\tColumn 0: Formation Energy\n",
      "\tColumn 1: E Above Hull\n",
      "\tColumn 2: Band Gap\n",
      "\tColumn 3: Nsites\n",
      "\tColumn 4: Density\n",
      "\tColumn 5: Volume\n",
      "\tColumn 6: Crystal System\n",
      "An example sample:\n",
      "\tFeature 0: Formation Energy=-2.555\n",
      "\tFeature 1: E Above Hull=0.069\n",
      "\tFeature 2: Band Gap=1.854\n",
      "\tFeature 3: Nsites=15\n",
      "\tFeature 4: Density=2.925\n",
      "\tFeature 5: Volume=179.798\n",
      "\tClass (Family): triclinic\n",
      "The training set consists of 189 samples and 189 associated ground truth classes\n",
      "The test set consists of 48 samples and 48 associated ground truth classes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import count\n",
    "\n",
    "numerical_data = pd.read_csv(r'batteries.csv')\n",
    "\n",
    "print(\"The dataset consists of {} samples\".format(len(numerical_data)))\n",
    "print(\"The numerical data loaded consists of following columns:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tColumn {}: {}\".format(i, x) for i, x in enumerate(numerical_data.columns)])))\n",
    "print(\"An example sample:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tFeature {}: {}={}\".format(i, x, v) for i, (x, v) in \n",
    "               enumerate(zip(numerical_data.columns, numerical_data.loc[0, :])) if not x == 'Crystal System'])))\n",
    "print('\\tClass (Family): {}'.format(numerical_data.loc[0, 'Crystal System']))\n",
    "\n",
    "# We split the dataset into features and the target class, using the 'Family' column as the target:\n",
    "X_nd = numerical_data.loc[:, numerical_data.columns!='Crystal System'].values\n",
    "y_nd = numerical_data['Crystal System'].values\n",
    "\n",
    "# We further split the dataset into a training and testing set\n",
    "X_nd_train, X_nd_test, y_nd_train, y_nd_test = train_test_split(X_nd, y_nd, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"The training set consists of {} samples and {} associated ground truth classes\".format(len(X_nd_train),\n",
    "                                                                                             len(y_nd_train)))\n",
    "print(\"The test set consists of {} samples and {} associated ground truth classes\".format(len(X_nd_test),\n",
    "                                                                                             len(y_nd_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical data\n",
    "\n",
    "The second dataset for this assessment contains a mix of numerical and categorical features, with the goal of predicting the forest cover type. The data used in this assessment is adapted from a full dataset provided [here](https://archive.ics.uci.edu/ml/datasets/Covertype) (Jock A. Blackard and Colorado State University).\n",
    "\n",
    "Each sample corresponds to a 30x30 meter cell, and consists of cartographic variables only (i.e. it does not rely on any remotely sensed imaging data), derived from from US Geological Survey (USGS) and USFS data. Each sample consists of the following features (categorical features can be distinguished by being encoded as a string rather tha a numer):\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Elevation`       | `float`: kilometers | Elevation |\n",
    "| `Aspect`   | `int`: degrees azimut | Aspect in degreez azimuth |\n",
    "| `Slope`      | `int`: degrees | Slope in degrees |\n",
    "| `Horizontal_Distance_To_Hydrology`   | `float`: kilometers | Horiz. distance to nearest surface water feature        |\n",
    "| `Vertical_Distance_To_Hydrology`   | `float`: kilometers | Vert. distance to nearest surface water feature        |\n",
    "| `Horizontal_Distance_To_Roadways`     | `float`: kilometers | Hor. distance to nearest roadway       |\n",
    "| `Hillshade_9am`   |`int`: $0 \\dots 255 $|  Hillshade index at 9am, summer solstice        |\n",
    "| `Hillshade_Noon`      |`int`: $0 \\dots 255 $| Hillshade index at noon, summer solstice       |\n",
    "| `Hillshade_3pm`   |`int`: $0 \\dots 255 $| Hillshade index at 3pm, summer solstice        |\n",
    "| `Horizontal_Distance_To_Fire_Points`  | `float`: kilometers | Horiz. distance to nearest wildfire ignition point        |\n",
    "| `Wilderness_Area`   | `string`: name | Wilderness area name (4 different values)       |\n",
    "| `Soil_Type`   |`string`: code| Soil type code (40 different values)       |\n",
    "\n",
    "The goal of this dataset is to predict the forrest cover type for each 30x30 meter cell. A class, corresponding to the forest cover type, is associated to each sample:\n",
    "\n",
    "| Class      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `Cover_Type`       | `string`: name | Forest cover type designation (7 different values). |\n",
    "\n",
    "\n",
    "**Note:** Tackling the categorical dataset is a **stretch task** for this assessment. It will be used in the development and testing of the [Decision Tree](#Decision-Trees) classifier, as well as for their [evaluation](#Task-4.1---Evaluation). Tackling this data will **require you to implement a** `DecisionTree` **from scratch**, as `sklearn` **does not provide this functionality**.\n",
    "\n",
    "The following cell loads the dataset, and splits the samples randomly, using  $80\\%$ of samples for training and $20\\%$ for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists of 58101 samples\n",
      "The numerical data loaded consists of following columns:\n",
      "\tColumn 0: Elevation\n",
      "\tColumn 1: Aspect\n",
      "\tColumn 2: Slope\n",
      "\tColumn 3: Horizontal_Distance_To_Hydrology\n",
      "\tColumn 4: Vertical_Distance_To_Hydrology\n",
      "\tColumn 5: Horizontal_Distance_To_Roadways\n",
      "\tColumn 6: Hillshade_9am\n",
      "\tColumn 7: Hillshade_Noon\n",
      "\tColumn 8: Hillshade_3pm\n",
      "\tColumn 9: Horizontal_Distance_To_Fire_Points\n",
      "\tColumn 10: Wilderness_Area\n",
      "\tColumn 11: Soil_Type\n",
      "\tColumn 12: Cover_Type\n",
      "An example sample:\n",
      "\tFeature 0: Elevation=3.0\n",
      "\tFeature 1: Aspect=87.0\n",
      "\tFeature 2: Slope=9.0\n",
      "\tFeature 3: Horizontal_Distance_To_Hydrology=0.3\n",
      "\tFeature 4: Vertical_Distance_To_Hydrology=0.0\n",
      "\tFeature 5: Horizontal_Distance_To_Roadways=3.7\n",
      "\tFeature 6: Hillshade_9am=233.0\n",
      "\tFeature 7: Hillshade_Noon=226.0\n",
      "\tFeature 8: Hillshade_3pm=125.0\n",
      "\tFeature 9: Horizontal_Distance_To_Fire_Points=2.2\n",
      "\tFeature 10: Wilderness_Area=Rawah\n",
      "\tFeature 11: Soil_Type=ELU-7745\n",
      "\tClass (Cover_Type): Lodgepole_Pine\n",
      "The training set consists of 46480 samples and 46480 associated ground truth classes\n",
      "The test set consists of 11621 samples and 11621 associated ground truth classes\n"
     ]
    }
   ],
   "source": [
    "categorical_data = pd.read_csv(r'forestcover.csv')\n",
    "\n",
    "print(\"The dataset consists of {} samples\".format(len(categorical_data)))\n",
    "print(\"The numerical data loaded consists of following columns:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tColumn {}: {}\".format(i, x) for i, x in enumerate(categorical_data.columns)])))\n",
    "print(\"An example sample:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\tFeature {}: {}={}\".format(i, x, v) for i, (x, v) in \n",
    "               enumerate(zip(categorical_data.columns, categorical_data.loc[0, :])) if not x == 'Cover_Type'])))\n",
    "print('\\tClass (Cover_Type): {}'.format(categorical_data.loc[0, 'Cover_Type']))\n",
    "\n",
    "# We split the dataset into features and the target class, using the 'Cover_Type' column as the target:\n",
    "X_cd = categorical_data.loc[:, categorical_data.columns!='Cover_Type'].values\n",
    "y_cd = categorical_data['Cover_Type'].values\n",
    "\n",
    "\n",
    "# We further split the dataset into a training and testing set\n",
    "X_cd_train, X_cd_test, y_cd_train, y_cd_test = train_test_split(X_cd, y_cd, test_size = 0.2, random_state = 0)\n",
    "\n",
    "print(\"The training set consists of {} samples and {} associated ground truth classes\".format(len(X_cd_train),\n",
    "                                                                                             len(y_cd_train)))\n",
    "print(\"The test set consists of {} samples and {} associated ground truth classes\".format(len(X_cd_test),\n",
    "                                                                                             len(y_cd_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Dataset statistics\n",
    "#### 10% marks\n",
    "\n",
    "To handle the data for the classification tasks, implement functions that will assist in querying some basic information about the dataset you are handling.\n",
    "\n",
    "Given a set of M samples $\\mathbf{x} = \\{\\mathbf{x}_i\\} \\forall i=1..M$ with n features each $\\mathbf{x}_i = \\{x_{1,i}, \\dots, x_{n,i}\\}$, and the corresponding ground truth labels $\\mathbf{y} = \\{y_i\\} \\forall i=1..M$, implement two functions:\n",
    "\n",
    "- `class_probabilities(y)` : calcualtes the class probabilities from the list of ground truth labels\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y`: The list of ground truth labels for a dataset<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    A dictionary of class labels and their probabilities (i.e. the frequency of that class in the dataset)\n",
    "\n",
    "    *Example* :\n",
    "    ```python\n",
    "    y = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "    print(class_probabilities(y))\n",
    "    ```\n",
    "    *Example output*:<br>\n",
    "    `{0: 0.3, 1: 0.5, 2: 0.2}`\n",
    "    \n",
    "    *Note:* Python dictionaries are unordered, so running this example might result in a different order of keys in the output, for example: `{2: 0.2, 0: 0.3, 1: 0.5}`<br><br>\n",
    "   \n",
    "- `most_common_class` : finds the most common class from the list of ground truth class labels.\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y`: the list of ground truth labels for a dataset<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    the label of the most common class represented in the dataset.\n",
    "    \n",
    "    *Example*:\n",
    "    ```python\n",
    "    y = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "    print(most_common_class(y))\n",
    "    ```\n",
    "    *Example output*:<br>\n",
    "    `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_totals(y):\n",
    "    classTotals = {} #create instance of classTotals\n",
    "\n",
    "    for i in range(len(y)): # loop through all the labels\n",
    "        if y[i] not in classTotals: # if there is no instance of a class in the dictionary, create one and set it's value to 1\n",
    "            classTotals[y[i]] = 1 \n",
    "        else: #if there is an instance increment its value\n",
    "            classTotals[y[i]] += 1\n",
    "    \n",
    "    return classTotals #return final dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(y):\n",
    "    classTotals = class_totals(y) # set classTotal to the what is returned from the class_totals method \n",
    "    classProbs = {} # create an instance of classProbs \n",
    "\n",
    "    for i in classTotals: # loop through all entries in dictionary, dividing their values by the total amount of classes \n",
    "        classProbs[i] = classTotals[i] / len(y)\n",
    "    \n",
    "    return classProbs #  return probabilities \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_class(y):\n",
    "    classTotals = class_totals(y) # set classTotal to the what is returned from the class_totals method \n",
    "    highestVal = 0 # create an instance of highestVal \n",
    "    for i in classTotals: # loop through all entries in dictionary, if the current entry is higher than the previous then set highestVal to the higher value\n",
    "        if classTotals[i] > highestVal:\n",
    "            highestVal = classTotals[i]\n",
    "        else: pass\n",
    "    for i in classTotals:\n",
    "        if highestVal == classTotals[i]:\n",
    "            return i # return the highest value \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique class labels and counts from numerical-only dataset: monoclinic: 92 orthorhombic: 89 triclinic: 56\n",
      "All classes and their probabilities:\n",
      "\ttriclinic: 0.23628691983122363\n",
      "\tmonoclinic: 0.3881856540084388\n",
      "\torthorhombic: 0.3755274261603376\n",
      "Most common class in the numerical-only dataset: monoclinic\n",
      "\n",
      "\n",
      "All unique class labels for the numerical-only training set: monoclinic: 76 orthorhombic: 69 triclinic: 44\n",
      "All classes and their probabilities (num-only training set):\n",
      "\ttriclinic: 0.2328042328042328\n",
      "\tmonoclinic: 0.4021164021164021\n",
      "\torthorhombic: 0.36507936507936506\n",
      "Most common class in the num-only training set: monoclinic\n",
      "\n",
      "\n",
      "All unique class labels for the numerical-only testing set: monoclinic: 16 orthorhombic: 20 triclinic: 12\n",
      "All classes and their probabilities (num-only tests set):\n",
      "\ttriclinic: 0.25\n",
      "\tmonoclinic: 0.3333333333333333\n",
      "\torthorhombic: 0.4166666666666667\n",
      "Most common class in the num-only test set: orthorhombic\n",
      "\n",
      "\n",
      "All class labels for a toy set: [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
      "All classses and their probabilities for the toy set:\n",
      "\t1: 0.5\n",
      "\t0: 0.3\n",
      "\t2: 0.2\n",
      "Most common class in the toy set: 1\n",
      "\n",
      "\n",
      "All classes and their probabilities from the categorical dataset:\n",
      "\tLodgepole_Pine: 0.486\n",
      "\tKrummholz: 0.035\n",
      "\tSpruce/Fir: 0.367\n",
      "\tPonderosa_Pine: 0.062\n",
      "\tDouglas-fir: 0.029\n",
      "\tAspen: 0.017\n",
      "\tCottonwood/Willow: 0.005\n",
      "Most common class in the categorical dataset: Lodgepole_Pine\n"
     ]
    }
   ],
   "source": [
    "unq, cnt = np.unique(y_nd, return_counts=True)\n",
    "print(\"All unique class labels and counts from numerical-only dataset: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd).items()])))\n",
    "print(\"Most common class in the numerical-only dataset: {}\".format(most_common_class(y_nd)))\n",
    "print(\"\\n\")\n",
    "\n",
    "unq, cnt = np.unique(y_nd_train, return_counts=True)\n",
    "print(\"All unique class labels for the numerical-only training set: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities (num-only training set):\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd_train).items()])))\n",
    "print(\"Most common class in the num-only training set: {}\".format(most_common_class(y_nd_train)))\n",
    "print(\"\\n\")\n",
    "\n",
    "unq, cnt = np.unique(y_nd_test, return_counts=True)\n",
    "print(\"All unique class labels for the numerical-only testing set: {}\".format(\n",
    "    ' '.join(['{}: {}'.format(v,c) for v, c in zip(unq, cnt)])))\n",
    "print(\"All classes and their probabilities (num-only tests set):\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_nd_test).items()])))\n",
    "print(\"Most common class in the num-only test set: {}\".format(most_common_class(y_nd_test)))\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test = [1, 0, 0, 0, 1, 2, 1, 1, 2, 1]\n",
    "print(\"All class labels for a toy set: {}\".format(y_test))\n",
    "print(\"All classses and their probabilities for the toy set:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {}\".format(c, p) for c, p in class_probabilities(y_test).items()])))\n",
    "print(\"Most common class in the toy set: {}\".format(most_common_class(y_test)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"All classes and their probabilities from the categorical dataset:\\n{}\".format(\n",
    "    \"\\n\".join([\"\\t{}: {:.3f}\".format(c, p) for c, p in class_probabilities(y_cd).items()])))\n",
    "print(\"Most common class in the categorical dataset: {}\".format(most_common_class(y_cd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Euclidean distance\n",
    "#### 10% marks\n",
    "\n",
    "One of the most commonly used distance metrics is the Euclidean distance. The Euclidean distance between two points in the Euclidean space (also known as $L_2$ norm) is defined as the length of the line segment between those two points. For example, if we have two points in 2D space (i.e. two samples consisting of two features), $\\mathbf{x}_0 = \\{x_{0,0}, x_{0,1}\\}$ and $\\mathbf{x}_1 = \\{x_{1,0}, x_{1,1}\\}$, we can calculate the distance as:\n",
    "\n",
    "\\begin{equation*}\n",
    "d_{\\texttt{Euclidean}} = \\sqrt{(x_{0,0} - x_{1,0})^2 + (x_{0,1} - x_{1,1})^2}\n",
    "\\end{equation*}\n",
    "\n",
    "In other words, we need to calculate the difference between the first features of the two samples, the difference between the second features of the two samples, then square each of the differences, sum those squares, and calculate the square root of the resulting sum.\n",
    "\n",
    "In general, when we are dealing with points in nD space (corresponding to samples with n features) of the form $\\mathbf{x}_i = \\{x_{0,i}, x_{1,i}, \\dots, x_{n,i}\\}$, the procedure is very similar. To calculade the Euclidean disatnce, you need to calculate the difference between the corresponding features of two samples, square each of the differences, and calculate the square root of the resulting sum:\n",
    "\n",
    "\\begin{equation*}\n",
    "d_{\\texttt{Euclidean}} = \\sqrt{\\sum_i{(x_{0,i} - x_{1,i})^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "Implement the Euclidean distance as a Python function:\n",
    "- `euclidean_distance`: calculates the Euclidean distance between two n-dimensional samples\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `x1`: An n-dimensional sample. This either a list of length $n$, or a `numpy.array` of dimensions `(n,)`<br>\n",
    "    `x2`: An n-dimensional sample. This either a list of length $n$, or a `numpy.array` of dimensions `(n,)`<br>\n",
    "\n",
    "    *Returns*<br>\n",
    "    The Euclidean distance between `x1` and `x2`.<br>\n",
    "    \n",
    "    *Example:*\n",
    "\n",
    "    ```python\n",
    "    \n",
    "    print(euclidean_distance(x1, x2)\n",
    "    ```\n",
    "    *Example output:*<br>\n",
    "    `5.0`\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbours\n",
    "\n",
    "For the first part of the assessment, you are required to implement **k Nearest Neigbours**. A key concept for kNN (and many different machine learning algorithms) is that of **distance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):            \n",
    "\n",
    "    euclideanDistance = 0 # create instance of euclideanDistance\n",
    "    for i in range(len(x1)): #loop through each value in both lists, we are assuming they will both be the same length\n",
    "        euclideanDistance += (x1[i] - x2[i])**2 # utilise the equation for euclidean distance \n",
    "\n",
    "    return math.sqrt(euclideanDistance) # return the square root of the our total to satisfy the equation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between [7, -1] (2D) and [4, 3] (2D) is 5.0\n",
      "Euclidean distance between [1, 2] (2D) and [3, 4] (2D) is 2.828\n",
      "Euclidean distance between [4.2, 3.6, 1.0] (3D) and [5.7, 2.8, -1.5] (3D) is 3.023\n",
      "Euclidean distance between [0.83058021 0.88146154 0.51377642 0.12487268 0.42650771 0.56101245\n",
      " 0.37558681 0.89906491 0.34397313 0.27198872 0.60728567 0.95335926\n",
      " 0.75608421 0.35301584 0.94934458 0.41587546 0.83642014 0.74613035\n",
      " 0.05734365 0.94500225] (20D) and [0.59493345 0.89315558 0.56226419 0.38097654 0.31565233 0.51689492\n",
      " 0.60335192 0.30560652 0.48948783 0.52137715 0.43175516 0.63848752\n",
      " 0.77519514 0.40615347 0.08627394 0.88372182 0.84938992 0.45115097\n",
      " 0.99236064 0.03227656] (20D) is 1.875\n"
     ]
    }
   ],
   "source": [
    "a = [7, -1]\n",
    "b = [4, 3]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = [1, 2]\n",
    "b = [3, 4]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = [4.2, 3.6, 1.0]\n",
    "b = [5.7, 2.8, -1.5]\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))\n",
    "a = np.random.rand(20)\n",
    "b = np.random.rand(20)\n",
    "print(\"Euclidean distance between {} ({}D) and {} ({}D) is {:.3f}\".format(\n",
    "    a, len(a), b, len(b), euclidean_distance(a,b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - kNN classifier\n",
    "#### 20% marks\n",
    "\n",
    "K nearest neighbours classifier will predict the class of a sample based on the class label of its $k$ nearest neigbours, according to a given distance metric. Implement `kNN` classifier which works on samples with numerical features, and relies on the Euclidean distance, as a Python class implementing the following member functions:\n",
    "\n",
    "- `__init__(self, k = 5, distance = euclidean_distance)`: class constructor\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `k` : The number of neigbours to be considered by the classification model <br>\n",
    "    `distance`: Tunction which is used to calculate the distance between samples (you will use the Euclidean distance implemented in the previous task)<br><br>\n",
    "    \n",
    "- `fit(self, X, y)`: member function used to train the classifier (fit the model to the data)\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The dataset samples. This is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, n)`<br>\n",
    "    `y`: The dataset labels. This is either a list of length $M$, or a `numpy.array` of dimensions `(M,)`<br><br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    The trained classifier model.<br><br>\n",
    "    \n",
    "- `predict(self, X)`: member function used to predict the classes of samples `X`\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The samples for which to predict a class. If this is a single sample, then `X` is either a list of length $n$, or a `numpy.array` of dimensions `(n,)`. If this is a set of samples, then `X` is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, N)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    Predicted class for the samples of X. If `X` was a single sample, then the output is a single class label. If `X` was a set of samples, then the output is a `numpy.array` of dimensions `(n,`)<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    The provided code outline already handles the case where `X` is a set of samples (by running on each sample separately). You need write the main logic of this function, i.e. calculating the prediction for a single sample.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class knn:\n",
    "    def __init__(self, k = 5, distance = euclidean_distance):\n",
    "        \n",
    "        self._k = k\n",
    "        self._distance = distance\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.X_Train = X\n",
    "        self.Y_Train = y\n",
    "\n",
    "        return self # this should probably be the last line in your fit function\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # the fololwing block of code handles lists/arrays containing multiple samples\n",
    "        X = np.array(X)\n",
    "        if len(X.shape) >= 2:\n",
    "            return np.array([self.predict(x) for x in X])\n",
    "\n",
    "        # in the following section, you can assume that X is a single n-dimensional sample:\n",
    "        \n",
    "        #select k\n",
    "        k = self._k\n",
    "        #find euclidean distance\n",
    "        distances = []\n",
    "        for i in range(len(self.X_Train)):\n",
    "            distances.append((self._distance(X, self.X_Train[i]), self.Y_Train[i])) # append the distance from test to current train row and it's class [dist,class]\n",
    "        #sort distances\n",
    "        distances = sorted(distances)\n",
    "        # take the label value of the k closest classes\n",
    "        neighbors = []\n",
    "        for i in range(k):\n",
    "            neighbors.append(distances[i][1]) \n",
    "\n",
    "        #find most common neighbor \n",
    "        prediction = self.most_common_class(neighbors) # find which class laebl appears the most \n",
    "        #make prediction\n",
    "        return prediction \n",
    "    \n",
    "    def class_totals(self, y):\n",
    "        classTotals = {} #create instance of classTotals\n",
    "\n",
    "        for i in range(len(y)): # loop through all the labels\n",
    "            if y[i] not in classTotals: # if there is no instance of a class in the dictionary, create one and set it's value to 1\n",
    "                classTotals[y[i]] = 1 \n",
    "            else: #if there is an instance increment its value\n",
    "                classTotals[y[i]] += 1\n",
    "\n",
    "        return classTotals #return final dictionary \n",
    "\n",
    "    def most_common_class(self, y):\n",
    "        classTotals = self.class_totals(y) # set classTotal to the what is returned from the class_totals method \n",
    "        highestVal = 0 # create an instance of highestVal \n",
    "        for i in classTotals: # loop through all entries in dictionary, if the current entry is higher than the previous then set highestVal to the higher value\n",
    "            if classTotals[i] > highestVal:\n",
    "                highestVal = classTotals[i]\n",
    "            else: pass\n",
    "\n",
    "        for i in classTotals:\n",
    "            if highestVal == classTotals[i]:\n",
    "                return i # return the highest value \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [-2.45500e+00  7.00000e-02  2.34400e+00  2.60000e+01  3.21300e+00\n",
      "  3.09993e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.35200e+00  8.30000e-02  1.36700e+00  2.80000e+01  2.93500e+00\n",
      "  3.57496e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.55300e+00  7.30000e-02  2.64900e+00  3.20000e+01  2.87700e+00\n",
      "  3.73622e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.43900e+00  5.70000e-02  2.30000e-01  1.50000e+01  3.02400e+00\n",
      "  1.77312e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: triclinic\n",
      "Sample: [-2.88700e+00  4.00000e-02  3.14400e+00  5.20000e+01  2.69000e+00\n",
      "  6.79101e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.56400e+00  5.80000e-02  2.73000e+00  2.80000e+01  2.85600e+00\n",
      "  3.60121e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.91100e+00  6.40000e-02  3.07900e+00  3.40000e+01  2.63300e+00\n",
      "  4.31422e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: triclinic\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier with k=3 (using 9 neigbours)\n",
    "classifier = knn(k=9)\n",
    "# Fit the classifier to the training set\n",
    "classifier.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_nd_test[3:10], classifier.predict(X_nd_test[3:10]), y_nd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "For the second part of the assignment, you are expected to implement a decision tree classifier. A decision tree models the data as a series of decisions based on the values of different features of the sample. The predictions are made in an interpretable fashion, where the decision is made by considering different sample features, and their values, until the new sample is grouped with similar samples presented during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 - Entropy and Information Gain\n",
    "#### 10% marks\n",
    "\n",
    "Important concepts in decision trees are **entropy** and **information gain**. **Entropy** is a measure of the variance in the data, of how \"unpredictable\" the dataset is:\n",
    "\n",
    "- If a dataset consists only of samples with the same label (e.g. $\\{\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare\\}$), the entropy will be zero (the data is predictable).\n",
    "- If a dataset consists of an equal amount of samples from two classes, (e.g. $\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$), the entropy will be one (the data is unpredictable).\n",
    "- If the class distribution in the dataset lies somewhere between these two extremes, the entropy will be between one and zero.\n",
    "- Entropy also increases as the number of different classes increases.\n",
    "\n",
    "Given $c$ classes, and their associated probabilities (frequencies) in the datasets, $p_i \\forall i \\in 0..c$, the entropy is calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "E = -\\sum_i p_i log_2(p_i).\n",
    "\\end{equation*}\n",
    "\n",
    "Implement a function `entropy` which calculates the entropy of a set of class labels from the list of their associated probabilities:\n",
    "\n",
    "- `entropy(p_per_class)`: calculates the entropy of a set of class probabilities\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `p_per_class`: A list of $c$ probabilities, one for each class in the dataset. This either a list of length $c$, or a `numpy.array` of dimensions `(c,)`<br>\n",
    "\n",
    "    *Returns:*<br>\n",
    "    The entropy of a set with class probabilities given in `p_per_class`.<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    You do not need to check if the list of probabilities given is correct; i.e. you may assume that all the probabilities in the list are strictly greater than zero, and that the sum of all the class probabilities equals one.\n",
    "    \n",
    "    *Example:*\n",
    "\n",
    "    ```python\n",
    "    probabilities = [0.5, 0.5]\n",
    "    print(entropy(probabilities))\n",
    "    ```\n",
    "    *Example output:*<br>\n",
    "    `1.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p_per_class):\n",
    "    entropy = 0\n",
    "    for i in p_per_class:\n",
    "        entropy += i*math.log2(i)\n",
    "    return -entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of a set with only one class: -0.0\n",
      "Entropy of a set with two equally probable classes: 1.0\n",
      "Entropy of a set with four equally probable clasess: 2.0\n",
      "Entropy of a set with four classes with different probabilities: 1.846\n",
      "Entropy of the numerical (batteries) dataset: 1.552\n",
      "Entropy of the categorical (forest cover) dataset: 1.739\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of a set with only one class: {}\".format(entropy([1.0])))\n",
    "print(\"Entropy of a set with two equally probable classes: {}\".format(entropy([0.5, 0.5])))\n",
    "print(\"Entropy of a set with four equally probable clasess: {}\".format(entropy([0.25, 0.25, 0.25, 0.25])))\n",
    "print(\"Entropy of a set with four classes with different probabilities: {:.3f}\".format(\n",
    "    entropy([0.2, 0.3, 0.4, 0.1])))\n",
    "print(\"Entropy of the numerical (batteries) dataset: {:.3f}\".format(\n",
    "    entropy(list(class_probabilities(y_nd).values()))))\n",
    "print(\"Entropy of the categorical (forest cover) dataset: {:.3f}\".format(\n",
    "    entropy(list(class_probabilities(y_cd).values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **information gain** describes how much variance was lost by dividing a set into parts, i.e. how much more \"predictable\" the parts are from the whole.\n",
    "\n",
    "For example, if we have a dataset $d=\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare,\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$ (which is \"maximally unpredictable\", so we can calculate $E(d) = 1$), we can:\n",
    "- split it into $d_1=\\{\\blacksquare,\\blacksquare,\\blacksquare,\\blacksquare\\}$ and $d_2=\\{\\triangle, \\triangle, \\triangle, \\triangle\\}$.\n",
    "\n",
    "    Both of $d_1$ and $d_2$ are \"very predictable\" ($E(d_1) = E(d_2) = 0$), both subsets consist of one class only). The resulting information gain is large (we go from unpredictable to predictable, so we gain information):\n",
    "    \n",
    "    $I = E(d) - (0.5E(d_1) + 0.5E(d_2) = 1 - (0+0) =1$\n",
    "    \n",
    "- split it into $d_1=\\{\\blacksquare,\\triangle,\\triangle,\\blacksquare\\}$ and $d_2=\\{\\triangle,\\blacksquare,\\triangle,\\blacksquare\\}$.\n",
    "\n",
    "    Both of $d_1$ and $d_2$ are \"very unpredictable\" ($E(d_1) = E(d_2) = 1$), both subsets have the same amount of samples from each of the two classes). The resulting information gain is small (we start from an unpredictable dataset, and end with two unpredictable subsets, so we do not gain information):\n",
    "\n",
    "    $I = E(d) - (0.5E(d_1) + 0.5E(d_2) = 1 - (0.5\\times1+0.5\\times1) =0$\n",
    "    \n",
    "<br>\n",
    "In general, if we have a dataset $d$ containing $|d|$ samples, and we split it into $s$ subsets $d_i \\forall i=1..s$, each of size $|d_i|$, the information gain of this split is calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    I = E(d) - \\sum_{i=0}^s \\frac{|d_i|}{|d|}E(d_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 - Decision Tree classifier\n",
    "#### 30% marks\n",
    "\n",
    "In each decision node of a decision tree, this classifier splits the dataset into two or more parts according to a value of a certain feature in the dataset.\n",
    "\n",
    "Given a set of M samples $\\mathbf{x} = \\{\\mathbf{x}_i\\} \\forall i=1..M$ with n features each $\\mathbf{x}_i = \\{x_{1,i}, \\dots, x_{n,i}\\}$, we could decide to split the dataset at the $j^{\\texttt{th}}$ (numerical) feature at value $v$. This would mean splitting the dataset into:\n",
    "- the subset $\\mathbf{x_{<}} = \\{\\mathbf{x}_i | x_{j,i} < v\\}$, which contains all the samples with the $j^{\\texttt{th}}$ smaller than $v$ and\n",
    "- the subset $\\mathbf{x_{\\geq}} = \\{\\mathbf{x}_i | x_{j,i} \\geq v\\}$, which contains all the samples with the $j^{\\texttt{th}}$ greater or equal than $v$.\n",
    "\n",
    "Similarly, we could decide to split a dataset at the $j^{\\texttt{th}}$ feature which is categorical. If the $j^{\\texttt{th}}$ feature of a sample $\\mathbf{x}_i$ can hold values $\\{\\texttt{child}, \\texttt{teen}, \\texttt{adult}\\}$, the resulting subsets would be:\n",
    "- the subset $\\mathbf{x_{\\texttt{child}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{child}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{child}$, \n",
    "- the subset $\\mathbf{x_{\\texttt{teen}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{teen}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{teen}$ and\n",
    "- the subset $\\mathbf{x_{\\texttt{adult}}} = \\{\\mathbf{x}_i | x_{j,i} = \\texttt{adult}\\}$, which contains all the samples with the feature $j^{\\texttt{th}}$ equal to $\\texttt{adult}$.\n",
    "\n",
    "In each node of the decision tree, the dataset is split according to the attribute $j$ which results in **the highest information gain** after the split. For categorical features, there is only one possible split (each category becomes a subset). For numerical features, it is neccessary to check all the possible values of split. For example, if the $j^{\\texttt{th}}$ in the dataset has appeared with the values of $\\{1, 1.5, 2.7, 3.2, 5\\}$, you need to calculate the information gain for splitting the dataset at $v=1.5$ (into $\\mathbf{x_{< 1.5}}$ and $\\mathbf{x_{\\geq 1.5}}$), $v=2.7$, $v=3.2$, $v=5$.\n",
    "\n",
    "The splitting continues until each node contains only samples of a single class, or a stopping criterion is reached. If a node contains training samples of more than one class, it returns the label of the majority (most probable) class as its decision.\n",
    "\n",
    "Implement a `DecisionTree` classifier which works on samples with both numerical and categorical features as a Python class implementing the following member functions:\n",
    "\n",
    "- `__init__(self, max_depth = -1, min_samples_per_node = 1)`: class constructor\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `max_dept` : The maximal depth of the tree (`max_depth == 0` corresponds to a decision tree with only the root node. `max_depth == -1` means no limit to the depth of the tree) <br>\n",
    "    `min_samples_per_node`: The minimal number of samples contained in a terminal (decision) node. If a split would cause one of the subsets to have less than `min_samples_per_node` samples, the split is not performed. <br><br>\n",
    "    \n",
    "- `fit(self, X, y)`: member function used to train the classifier (fit the model to the data)\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The dataset samples. This is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, n)`<br>\n",
    "    `y`: The dataset labels. This is either a list of length $M$, or a `numpy.array` of dimensions `(M,)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    The trained classifier model.<br><br>\n",
    "    \n",
    "- `predict(self, X)`: member function used to predict the classes of samples `X`\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `X`: The samples for which to predict a class. If this is a single sample, then `X` is either a list of length $n$, or a `numpy.array` of dimensions `(n,)`. If this is a set of samples, then `X` is either a list of $M$ lists of length $n$, or a `numpy.array` of dimensions `(M, N)`<br>\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    Predicted class for the samples of X. If `X` was a single sample, then the output is a single class label. If `X` was a set of samples, then the output is a `numpy.array` of dimensions `(n,`)<br>\n",
    "    \n",
    "    *Note:*<br>\n",
    "    The provided code outline already handles the case where `X` is a set of samples (by running on each sample separately). You need write the main logic of this function, i.e. calculating the prediction for a single sample.<br><br>\n",
    "    \n",
    "*Note:* Your implementation will be tested in two parts. Firstly, it will be tested only on a dataset containing numerical values. Secondly, it will also be tested on a dataset containing a mix of categorical and numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_nd_train\n",
    "y = y_nd_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each column represents each of the samples and the rows represent the features of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.94600e+00,  2.00000e-02,  3.28600e+00,  4.40000e+01,\n",
       "         3.00900e+00,  5.21131e+02],\n",
       "       [-2.60300e+00,  2.40000e-02,  2.93100e+00,  1.60000e+01,\n",
       "         3.06000e+00,  1.75643e+02],\n",
       "       [-2.64700e+00,  7.20000e-02,  2.37300e+00,  4.10000e+01,\n",
       "         2.50100e+00,  5.93870e+02],\n",
       "       ...,\n",
       "       [-2.59900e+00,  2.30000e-02,  4.01000e-01,  1.40000e+01,\n",
       "         2.87000e+00,  1.79222e+02],\n",
       "       [-2.76900e+00,  7.70000e-02,  3.18800e+00,  6.40000e+01,\n",
       "         2.51700e+00,  9.29064e+02],\n",
       "       [-2.66900e+00,  7.20000e-02,  2.88300e+00,  2.20000e+01,\n",
       "         2.94700e+00,  2.53565e+02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.946, -2.603, -2.647, -2.56 , -2.837, -2.566, -2.598, -2.785,\n",
       "       -2.601, -2.706, -2.802, -2.291, -2.626, -2.598, -2.349, -2.495,\n",
       "       -2.324, -2.537, -2.374, -2.691, -2.603, -2.768, -2.868, -2.836,\n",
       "       -2.942, -2.958, -2.566, -2.784, -2.522, -2.608, -2.532, -2.608,\n",
       "       -2.858, -2.677, -2.695, -2.142, -2.481, -2.73 , -2.842, -2.66 ,\n",
       "       -2.301, -2.553, -2.69 , -2.524, -2.539, -2.603, -2.564, -2.764,\n",
       "       -2.539, -2.89 , -2.591, -2.418, -2.791, -2.467, -2.574, -2.939,\n",
       "       -2.339, -2.607, -2.78 , -2.653, -2.581, -2.585, -2.743, -2.691,\n",
       "       -2.474, -2.332, -2.564, -2.438, -2.607, -2.65 , -2.369, -2.58 ,\n",
       "       -2.588, -2.37 , -2.589, -2.546, -2.433, -2.876, -2.172, -2.971,\n",
       "       -2.927, -2.553, -2.782, -2.598, -2.356, -2.523, -2.577, -2.555,\n",
       "       -2.613, -2.646, -2.981, -2.955, -2.551, -2.444, -2.431, -2.598,\n",
       "       -2.871, -2.571, -2.785, -2.132, -2.868, -2.563, -2.722, -2.872,\n",
       "       -2.687, -2.496, -2.847, -2.646, -2.369, -2.545, -2.77 , -2.353,\n",
       "       -2.605, -2.791, -2.827, -2.565, -2.817, -2.439, -2.556, -2.623,\n",
       "       -2.598, -2.486, -2.896, -2.347, -2.775, -2.527, -2.68 , -2.441,\n",
       "       -2.25 , -2.604, -2.439, -2.694, -2.62 , -2.61 , -2.563, -2.597,\n",
       "       -2.452, -2.985, -2.824, -2.621, -2.779, -2.626, -2.371, -2.694,\n",
       "       -2.582, -2.621, -2.595, -2.482, -2.436, -2.832, -2.528, -2.685,\n",
       "       -2.904, -2.144, -2.619, -2.976, -2.758, -2.352, -2.604, -2.721,\n",
       "       -2.69 , -2.823, -2.747, -2.95 , -2.565, -2.618, -2.833, -2.898,\n",
       "       -2.551, -2.632, -2.597, -2.762, -2.529, -2.632, -2.339, -2.391,\n",
       "       -2.343, -2.783, -2.598, -2.329, -2.372, -2.638, -2.561, -2.418,\n",
       "       -2.56 , -2.615, -2.599, -2.769, -2.669])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0] # samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.94600e+00,  2.00000e-02,  3.28600e+00,  4.40000e+01,\n",
       "        3.00900e+00,  5.21131e+02])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0,:] # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 6 Total samples: 189 Total labels: 189\n"
     ]
    }
   ],
   "source": [
    "# information gain\n",
    "totalSamples, totalFeatures = X.shape\n",
    "totalLabels = len(y)\n",
    "\n",
    "print(f\"Total features: {totalFeatures} Total samples: {totalSamples} Total labels: {totalLabels}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find best split**\n",
    "\n",
    "1. isolate each feature column \n",
    "2. isolate each threshold in said feature column \n",
    "3. find the information gain of each threshold \n",
    "    1. find the entropy of the parent calsses\n",
    "    2. create a theoretical split at the threshold\n",
    "    3. calculate the entropy of the left and right side of the split \n",
    "    4. take the split entropy from the parent entropy \n",
    "    5. retrun value \n",
    "4. split at the threshold with the best information gain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_probabilities(y):\n",
    "    classTotals = class_totals(y) # set classTotal to the what is returned from the class_totals method \n",
    "    classProbsDict = {} # create an instance of classProbs \n",
    "    classProbsList = []\n",
    "    \n",
    "    for i in classTotals: # loop through all entries in dictionary, dividing their values by the total amount of classes \n",
    "        classProbsDict[i] = classTotals[i] / len(y)\n",
    "    \n",
    "    for i in classProbsDict:\n",
    "        classProbsList.append(classProbsDict[i])\n",
    "    \n",
    "    return classProbsList #  return probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X, threshold):\n",
    "    leftSplitIndex = np.argwhere(X <= threshold).flatten()\n",
    "    rightSplitIndex = np.argwhere(X > threshold).flatten()\n",
    "    return leftSplitIndex, rightSplitIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2328042328042328, 0.4021164021164021, 0.36507936507936506]\n"
     ]
    }
   ],
   "source": [
    "print(class_probabilities(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, threshold):\n",
    "    parentProbs = class_probabilities(y) \n",
    "    parentEntropy = entropy(parentProbs) # 1. find entropy of parent classes \n",
    "\n",
    "    leftSplit, rightSplit = split(X, threshold) # 2. create theoretical split at the threshold\n",
    "\n",
    "    totalClasses, totalLeft, totalRight = len(y), len(leftSplit), len(rightSplit)\n",
    "    \n",
    "    if totalLeft == 0 or totalRight == 0: #if the size of the left or right split is 0, return 0\n",
    "        return 0\n",
    "    \n",
    "    probsLeft, probsRight = class_probabilities(y[leftSplit]), class_probabilities(y[rightSplit])\n",
    "    \n",
    "    childEntropy = (totalLeft / totalClasses) * entropy(probsLeft) + (totalRight / totalClasses) * entropy(probsRight) # 3. calculate the entropy of the left and right split\n",
    "    \n",
    "    return parentEntropy - childEntropy # 4. take the split entropy from the parent entropy 5. return value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best split\n",
      "information gain: 0.13161640105034134\n",
      "feature: 5\n",
      "threshold: 687.797\n",
      "\n",
      "left: ['triclinic' 'monoclinic' 'triclinic' 'monoclinic' 'triclinic'\n",
      " 'orthorhombic' 'monoclinic' 'monoclinic' 'monoclinic' 'monoclinic'\n",
      " 'orthorhombic' 'orthorhombic' 'monoclinic' 'monoclinic' 'monoclinic'\n",
      " 'monoclinic' 'monoclinic' 'orthorhombic' 'orthorhombic' 'monoclinic'\n",
      " 'triclinic' 'monoclinic' 'triclinic' 'monoclinic' 'orthorhombic'\n",
      " 'orthorhombic' 'monoclinic' 'orthorhombic' 'orthorhombic' 'orthorhombic'\n",
      " 'monoclinic' 'monoclinic' 'triclinic' 'monoclinic' 'orthorhombic'\n",
      " 'monoclinic' 'monoclinic' 'monoclinic' 'orthorhombic' 'orthorhombic'\n",
      " 'monoclinic' 'monoclinic' 'orthorhombic' 'triclinic' 'orthorhombic'\n",
      " 'triclinic' 'monoclinic' 'monoclinic' 'monoclinic' 'orthorhombic'\n",
      " 'monoclinic' 'triclinic' 'orthorhombic' 'monoclinic' 'orthorhombic'\n",
      " 'monoclinic' 'monoclinic' 'triclinic' 'triclinic' 'monoclinic'\n",
      " 'monoclinic' 'triclinic' 'orthorhombic' 'monoclinic' 'triclinic'\n",
      " 'orthorhombic' 'triclinic' 'monoclinic' 'orthorhombic' 'monoclinic'\n",
      " 'monoclinic' 'monoclinic' 'triclinic' 'monoclinic' 'triclinic'\n",
      " 'monoclinic' 'triclinic' 'monoclinic' 'monoclinic' 'triclinic'\n",
      " 'triclinic' 'monoclinic' 'triclinic' 'orthorhombic' 'monoclinic'\n",
      " 'triclinic' 'monoclinic' 'triclinic' 'triclinic' 'monoclinic'\n",
      " 'monoclinic' 'triclinic' 'triclinic' 'monoclinic' 'orthorhombic'\n",
      " 'triclinic' 'triclinic' 'triclinic' 'orthorhombic' 'triclinic'\n",
      " 'orthorhombic' 'triclinic' 'triclinic' 'orthorhombic' 'orthorhombic'\n",
      " 'monoclinic' 'monoclinic' 'orthorhombic' 'orthorhombic' 'orthorhombic'\n",
      " 'triclinic' 'orthorhombic' 'monoclinic' 'triclinic' 'triclinic'\n",
      " 'monoclinic' 'monoclinic' 'monoclinic' 'orthorhombic' 'triclinic'\n",
      " 'triclinic' 'monoclinic' 'monoclinic' 'orthorhombic' 'monoclinic'\n",
      " 'triclinic' 'monoclinic' 'monoclinic' 'orthorhombic' 'triclinic'\n",
      " 'triclinic' 'triclinic' 'orthorhombic' 'triclinic' 'orthorhombic'\n",
      " 'orthorhombic' 'triclinic' 'triclinic' 'monoclinic' 'monoclinic'\n",
      " 'monoclinic' 'monoclinic' 'monoclinic' 'orthorhombic' 'monoclinic'\n",
      " 'orthorhombic' 'monoclinic' 'monoclinic']\n",
      "right: ['orthorhombic' 'orthorhombic' 'monoclinic' 'orthorhombic' 'monoclinic'\n",
      " 'monoclinic' 'orthorhombic' 'orthorhombic' 'monoclinic' 'monoclinic'\n",
      " 'orthorhombic' 'orthorhombic' 'orthorhombic' 'orthorhombic'\n",
      " 'orthorhombic' 'orthorhombic' 'orthorhombic' 'orthorhombic'\n",
      " 'orthorhombic' 'orthorhombic' 'orthorhombic' 'orthorhombic'\n",
      " 'orthorhombic' 'orthorhombic' 'orthorhombic' 'orthorhombic' 'triclinic'\n",
      " 'orthorhombic' 'orthorhombic' 'monoclinic' 'monoclinic' 'orthorhombic'\n",
      " 'orthorhombic' 'orthorhombic' 'monoclinic' 'orthorhombic' 'orthorhombic'\n",
      " 'monoclinic' 'orthorhombic' 'orthorhombic' 'monoclinic']\n"
     ]
    }
   ],
   "source": [
    "bestGain = {'gain': -1, 'feature': None, 'threshold': None}\n",
    "for feature in range(totalFeatures):\n",
    "    featureCol = X[:,feature] # take each individual feature column \n",
    "    thresholds = np.unique(featureCol) # take the unique values as thresholds \n",
    "    for threshold in thresholds:\n",
    "       gain =  information_gain(featureCol, y, threshold) # find the information gain of each threshold\n",
    "       if gain > bestGain['gain']:\n",
    "        bestGain['gain'] = gain\n",
    "        bestGain['feature'] = feature\n",
    "        bestGain['threshold'] = threshold\n",
    "\n",
    "print(f\"the best split\\ninformation gain: {bestGain['gain']}\\nfeature: {bestGain['feature']}\\nthreshold: {bestGain['threshold']}\")\n",
    "\n",
    "leftSplit, RightSplit = split(X[:,bestGain['feature']], bestGain['threshold'])\n",
    "print(f\"\\nleft: {y[leftSplit]}\\nright: {y[RightSplit]}\")\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(X, y, totalFeatures):\n",
    "    bestGain = {'gain': -1, 'feature': None, 'threshold': None}\n",
    "    for feature in range(totalFeatures):\n",
    "        featureCol = X[:,feature] # take each individual feature column \n",
    "        thresholds = np.unique(featureCol) # take the unique values as thresholds \n",
    "        for threshold in thresholds:\n",
    "           gain =  information_gain(featureCol, y, threshold) # find the information gain of each threshold\n",
    "           if gain > bestGain['gain']:\n",
    "            bestGain['gain'] = gain\n",
    "            bestGain['feature'] = feature\n",
    "            bestGain['threshold'] = threshold\n",
    "    return bestGain['feature'],bestGain['threshold']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have solved how to find the best splits we need to create a function that can recursivlely split the data set at the until only leaf nodes remain \n",
    "\n",
    "                0\n",
    "               / \\\n",
    "              -   0\n",
    "                 / \\\n",
    "                -   -               \n",
    "               \n",
    "1. Find best split with complete data set\n",
    "2. Split data at the best feature and threshold\n",
    "3. create children nodes holding each side of the split\n",
    "4. repeat process on each child until finishing criteria is met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDepth = 10\n",
    "minSamplesPerNode = 1\n",
    "\n",
    "def growTree(X, y, depth = 0):\n",
    "    totalSamples, totalFeatures = X.shape\n",
    "    totalLabels = len(y)\n",
    "    \n",
    "    if (depth >= maxDepth and maxDepth != -1) or totalLabels == 1 or totalSamples < minSamplesPerNode:\n",
    "        mostCommonClass = most_common_class(y)\n",
    "\n",
    "        return Node(value=mostCommonClass)\n",
    "    \n",
    "    bestFeature, bestThreshold = find_best_split(X, y, totalFeatures)\n",
    "\n",
    "    leftSplitIndexes, RightSplitIndexes = split(X[:,bestFeature], bestThreshold)\n",
    "    leftChild = growTree(X[leftSplitIndexes, :], y[leftSplitIndexes], depth + 1)\n",
    "    rightChild = growTree(X[RightSplitIndexes, :], y[RightSplitIndexes], depth + 1)\n",
    "\n",
    "    return Node(bestFeature, bestThreshold, leftChild, rightChild)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = growTree(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_class(y):\n",
    "        classTotals = class_totals(y) # set classTotal to the what is returned from the class_totals method \n",
    "        highestVal = 0 # create an instance of highestVal \n",
    "        for i in classTotals: # loop through all entries in dictionary, if the current entry is higher than the previous then set highestVal to the higher value\n",
    "            if classTotals[i] > highestVal:\n",
    "                highestVal = classTotals[i]\n",
    "            else: pass\n",
    "\n",
    "        for i in classTotals:\n",
    "            if highestVal == classTotals[i]:\n",
    "                return i # return the highest value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = -1, min_samples_per_node = 1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_per_node\n",
    "           \n",
    "    def isFinished(self, depth):\n",
    "        if (depth >= self.max_depth \n",
    "            or self.totalLabels == 1\n",
    "            or self.totalSamples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def bestSplit(self, x,y, totalFeatures):\n",
    "        bestGain = {'gain': -1, 'feature': None, 'threshold': None}\n",
    "        for feature in range(totalFeatures):\n",
    "            featureCol = X[:,feature] # take each individual feature column \n",
    "            thresholds = np.unique(featureCol) # take the unique values as thresholds \n",
    "            for threshold in thresholds:\n",
    "               gain =  information_gain(featureCol, y, threshold) # find the information gain of each threshold\n",
    "               if gain > bestGain['gain']:\n",
    "                bestGain['gain'] = gain\n",
    "                bestGain['feature'] = feature\n",
    "                bestGain['threshold'] = threshold\n",
    "        return bestGain['feature'],bestGain['threshold']\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.root = self.growTree(X, y)\n",
    "\n",
    "    def growTree(self, X, y, depth = 0):\n",
    "        totalSamples, totalFeatures = X.shape\n",
    "        totalLabels = len(y)\n",
    "\n",
    "        if (depth >= maxDepth and maxDepth != -1) or totalLabels == 1 or totalSamples < minSamplesPerNode:\n",
    "            mostCommonClass = most_common_class(y)\n",
    "\n",
    "            return Node(value=mostCommonClass)\n",
    "\n",
    "        bestFeature, bestThreshold = find_best_split(X, y, totalFeatures)\n",
    "\n",
    "        leftSplitIndexes, RightSplitIndexes = split(X[:,bestFeature], bestThreshold)\n",
    "        leftChild = self.growTree(X[leftSplitIndexes, :], y[leftSplitIndexes], depth + 1)\n",
    "        rightChild = self.growTree(X[RightSplitIndexes, :], y[RightSplitIndexes], depth + 1)\n",
    "\n",
    "        return Node(bestFeature, bestThreshold, leftChild, rightChild)\n",
    "    \n",
    "    def traverseTree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value \n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.traverseTree(x, node.left)\n",
    "        return self.traverseTree(x, node.right)\n",
    "        \n",
    "\n",
    "               \n",
    "    def predict(self, X):\n",
    "        predictions = [self.traverseTree(x, self.root) for x in X]\n",
    "        return np.array(predictions)\n",
    "        # in the following section, you can assume that X is a single n-dimensional sample:\n",
    "        ################################\n",
    "        ####### YOUR CODE HERE #########\n",
    "        ################################\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELLS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will test if the classifier implemented model can fit to the [numerical dataset](#Numerical-data), and predict solutions for some of the testing samples (this is not model evaluation yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [-2.45500e+00  7.00000e-02  2.34400e+00  2.60000e+01  3.21300e+00\n",
      "  3.09993e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.35200e+00  8.30000e-02  1.36700e+00  2.80000e+01  2.93500e+00\n",
      "  3.57496e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.55300e+00  7.30000e-02  2.64900e+00  3.20000e+01  2.87700e+00\n",
      "  3.73622e+02]\n",
      "\tPredicted class: triclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.43900e+00  5.70000e-02  2.30000e-01  1.50000e+01  3.02400e+00\n",
      "  1.77312e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: triclinic\n",
      "Sample: [-2.88700e+00  4.00000e-02  3.14400e+00  5.20000e+01  2.69000e+00\n",
      "  6.79101e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: monoclinic\n",
      "Sample: [-2.56400e+00  5.80000e-02  2.73000e+00  2.80000e+01  2.85600e+00\n",
      "  3.60121e+02]\n",
      "\tPredicted class: orthorhombic\n",
      "\tTrue class: orthorhombic\n",
      "Sample: [-2.91100e+00  6.40000e-02  3.07900e+00  3.40000e+01  2.63300e+00\n",
      "  4.31422e+02]\n",
      "\tPredicted class: monoclinic\n",
      "\tTrue class: triclinic\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier, requiring at least 3 samples in each node\n",
    "classifier = DecisionTree(min_samples_per_node = 3)\n",
    "# Fit the classifier to the training data\n",
    "classifier.fit(X_nd_train, y_nd_train)\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_nd_test[3:10], classifier.predict(X_nd_test[3:10]), y_nd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will check if the dataset is able to fit the model to the [dataset with mixed numerical and categorical values](#Categorical-data), and predict solutions for some of the testing samples (this is not model evaluation yet):\n",
    "\n",
    "_**Warning:**_ Executing this cell may take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: [2.4 48.0 11.0 0.0 0.0 0.4 224.0 215.0 123.0 1.3 'Cache_la_Poudre'\n",
      " 'ELU-2717']\n",
      "\tPredicted class: Ponderosa_Pine\n",
      "\tTrue class: Cottonwood/Willow\n",
      "Sample: [3.3 67.0 16.0 0.1 0.0 1.7 234.0 207.0 100.0 1.9 'Rawah' 'ELU-8772']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.3 60.0 15.0 0.1 -0.0 2.4 231.0 207.0 104.0 4.1 'Neota' 'ELU-8703']\n",
      "\tPredicted class: Krummholz\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.2 62.0 10.0 0.4 0.0 2.0 229.0 219.0 122.0 0.9 'Comanche_Peak'\n",
      " 'ELU-7755']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [2.9 121.0 6.0 0.5 0.1 3.6 230.0 235.0 139.0 3.1 'Rawah' 'ELU-4744']\n",
      "\tPredicted class: Lodgepole_Pine\n",
      "\tTrue class: Lodgepole_Pine\n",
      "Sample: [3.2 155.0 15.0 0.2 0.0 0.4 237.0 240.0 129.0 2.3 'Neota' 'ELU-4758']\n",
      "\tPredicted class: Spruce/Fir\n",
      "\tTrue class: Spruce/Fir\n",
      "Sample: [3.0 330.0 8.0 0.1 0.0 2.6 201.0 231.0 169.0 6.2 'Rawah' 'ELU-7745']\n",
      "\tPredicted class: Lodgepole_Pine\n",
      "\tTrue class: Spruce/Fir\n"
     ]
    }
   ],
   "source": [
    "# Initialise the classifier, requiring the decision tree to have no more than 5 levels\n",
    "classifier = DecisionTree(max_depth = 4)\n",
    "# Fit the classifier to the training data\n",
    "classifier.fit(X_cd_train, y_cd_train)\n",
    "\n",
    "# Predict the classification for some samples to the testing set\n",
    "for (sample, prediction, truth) in zip(X_cd_test[3:10], classifier.predict(X_cd_test[3:10]), y_cd_test[3:10]):\n",
    "    # Print the sample, predicted class and ground truth\n",
    "    print(\"Sample: {}\\n\\tPredicted class: {}\\n\\tTrue class: {}\".format(sample, prediction, truth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and analysis\n",
    "\n",
    "Executing all the cells from the previous section correctly only means that your implementation of [kNN classifier](#knn-Classifier) and [Decision Tree classifier](#Decision-Tree-classifier) runs on the provided data. This has still not provided any insight on how well those models represent the data.\n",
    "\n",
    "In this section, you are instead required to calculate some evaluation metrics, and evaluate the produced models on the whole of the held-out data (test set) defined in the [Dataset](#Dataset) section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 - Evaluation\n",
    "#### 10% marks\n",
    "\n",
    "You are required to write a function that will calculate different evaluation metrics based on the ground truth labels and the predicted labels. Specifically, write a function:\n",
    "- `evaluate(y_true, y_pred)`: evaluate classification results\n",
    "\n",
    "    *Inputs:*<br>\n",
    "    `y_true` : Ground truth labels for $n$ samples. This is either a list of $n$ elements, or a `numpy.array` of dimensions `(n,)`\n",
    "    `y_pred` : Predicted labels for $n$ samples. This is either a list of $n$ elements, or a `numpy.array` of dimensions `(n,)`\n",
    "    \n",
    "    *Returns:*<br>\n",
    "    A touple `(a, p, r, k)` consisting of the following values:\n",
    "    - `a` : overall accuracy of the predictions\n",
    "    - `p` : a dictionary containing a precision score for each class present in `y_true`\n",
    "    - `r` : a dictionary containing a recall score for each class present in `y_true`\n",
    "    - `k` : Cohen's Kappa metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOLUTION CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def precision_per_class(y_true, y_pred):\n",
    "    # Get the list of unique classes\n",
    "    classes = np.unique(y_true)\n",
    "    # Initialize a dictionary to store the precision scores for each class\n",
    "    scores = {}\n",
    "    # Iterate over the classes\n",
    "    for c in classes:\n",
    "        # Get the precision score for the current class\n",
    "        score = metrics.precision_score(y_true, y_pred, labels=[c], average='micro')\n",
    "        # Store the score in the dictionary\n",
    "        scores[c] = score\n",
    "    return scores\n",
    "\n",
    "def recall_per_class(y_true, y_pred):\n",
    "    # Get the list of unique classes\n",
    "    classes = np.unique(y_true)\n",
    "    # Initialize a dictionary to store the recall scores for each class\n",
    "    scores = {}\n",
    "    # Iterate over the classes\n",
    "    for c in classes:\n",
    "        # Get the recall score for the current class\n",
    "        score = metrics.recall_score(y_true, y_pred, labels=[c], average='micro')\n",
    "        # Store the score in the dictionary\n",
    "        scores[c] = score\n",
    "    return scores\n",
    "\n",
    "def evaluate(y_true, y_pred):\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    percision = precision_per_class(y_true, y_pred)\n",
    "\n",
    "    recall = recall_per_class(y_true, y_pred)\n",
    "\n",
    "    cohens = metrics.cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy,percision,recall,cohens # you may replace this line\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TESTING CELL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your implementation of [kNN classifier](#Task-2.2---kNN-classifier) and [Decision Tree classifier](#Task-3.2---Decision-Tree-classifier) on the numerical dataset defined in the [Dataset](#Dataset) section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating numerical dataset, using kNN:\n",
      "Accuracy = 0.5833, Cohen's Kappa = 0.3651\n",
      "\tClass monoclinic: precision = 0.56, recall = 0.62\n",
      "\tClass orthorhombic: precision = 0.67, recall = 0.60\n",
      "\tClass triclinic: precision = 0.50, recall = 0.50\n",
      "Evaluating numerical dataset, using DecisionTree:\n",
      "Accuracy = 0.5625, Cohen's Kappa = 0.3298\n",
      "\tClass monoclinic: precision = 0.45, recall = 0.62\n",
      "\tClass orthorhombic: precision = 0.65, recall = 0.55\n",
      "\tClass triclinic: precision = 0.67, recall = 0.50\n"
     ]
    }
   ],
   "source": [
    "classifier_knn = knn(k=5)\n",
    "classifier_knn.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "classifier_dt = DecisionTree(min_samples_per_node = 3)\n",
    "classifier_dt.fit(X_nd_train, y_nd_train)\n",
    "\n",
    "y_knn = classifier_knn.predict(X_nd_test)\n",
    "y_dt = classifier_dt.predict(X_nd_test)\n",
    "\n",
    "a_knn, p_knn, r_knn, k_knn = evaluate(y_nd_test, y_knn)\n",
    "a_dt, p_dt, r_dt, k_dt = evaluate(y_nd_test, y_dt)\n",
    "\n",
    "print(\"Evaluating numerical dataset, using kNN:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_knn, k_knn))\n",
    "for key in p_knn.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_knn[key], r_knn[key]))\n",
    "\n",
    "print(\"Evaluating numerical dataset, using DecisionTree:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_dt, k_dt))\n",
    "for key in p_dt.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_dt[key], r_dt[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your implementation of [Decision Tree classifier](#Decision-Tree-classifier) on the [dataset containing a mixture of numerical and categorical data](#Categorical-data):\n",
    "\n",
    "_**Warning:**_ Executing this cell may take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier_categorical \u001b[38;5;241m=\u001b[39m DecisionTree(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      2\u001b[0m classifier_categorical\u001b[38;5;241m.\u001b[39mfit(X_cd_train, y_cd_train)\n\u001b[1;32m----> 4\u001b[0m y_cd_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier_categorical\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_cd_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m a_cat, p_cat, r_cat, k_cat \u001b[38;5;241m=\u001b[39m evaluate(y_cd_test, y_cd_pred)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating categorical dataset, using DecisionTree:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 58\u001b[0m, in \u001b[0;36mDecisionTree.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 58\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(predictions)\n",
      "Cell \u001b[1;32mIn[29], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m---> 58\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X]\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(predictions)\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mright)\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mright)\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 52\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mright)\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping similar frames: DecisionTree.traverseTree at line 53 (1 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x[node\u001b[38;5;241m.\u001b[39mfeature] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraverseTree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 51\u001b[0m, in \u001b[0;36mDecisionTree.traverseTree\u001b[1;34m(self, x, node)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_leaf():\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraverseTree(x, node\u001b[38;5;241m.\u001b[39mright)\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "classifier_categorical = DecisionTree(max_depth = 4)\n",
    "classifier_categorical.fit(X_cd_train, y_cd_train)\n",
    "\n",
    "y_cd_pred = classifier_categorical.predict(X_cd_test)\n",
    "a_cat, p_cat, r_cat, k_cat = evaluate(y_cd_test, y_cd_pred)\n",
    "\n",
    "print(\"Evaluating categorical dataset, using DecisionTree:\")\n",
    "print(\"Accuracy = {:.4f}, Cohen's Kappa = {:.4f}\".format(a_cat, k_cat))\n",
    "for key in p_cat.keys():\n",
    "    print(\"\\tClass {}: precision = {:.2f}, recall = {:.2f}\".format(key, p_cat[key], r_cat[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 - Analysis\n",
    "#### 10% marks\n",
    "\n",
    "Answer the following questions in the space provided below (marked as **YOUR ANSWERS**):\n",
    "1. Explain the difference between the different performance metrics calculated for [Task 4.1](#Task-4.1---Evaluation). Which additional metrics could you propose to evaluate the performance of the defined models? Which of the calculated metrics would you chose to report, and why, for the analysis of the two [datasets](#Datasets) used in this assessment?\n",
    "\n",
    "2. In case you were handling a _very large_ amount of data, which of the models do you expect to be larger (i.e. take up more memory)? Which of the models do you expect to train faster? Which of the models do you expect to reach a decision faster? Explain and justify your answers.\n",
    "\n",
    "3. When handling numerical data, what is the role of normalisation? The numerical data used in this assessment was not normalised. If you were to normalise the data before training these classification models, would either of them change how they represent the data (and potentially return different results)?\n",
    "\n",
    "4. For evaluating the performance of the classifiers implemented for this assignment, $80\\%$ of the data was used for training the models, and $20\\%$ for testing. What are the shortcomings of this evaluation strategy, and can you propose and describe a better one?\n",
    "\n",
    "_Note:_ You may use the cell marked as **EXPERIMENTAL CELL** to write any code that could help you answer the above questions, or in general, to experiment with the code produced for this assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPERIMENTAL CELL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ You may use this cell to write any code which may help you answers the questions above ##############\n",
    "####### Your implementations from this cell will not be assessed, feel free to use it for experimentation ########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR ANSWERS:\n",
    "\n",
    "1. <font color='red'>Answer to question 1</font> \n",
    "2. <font color='red'>Answer to question 2</font> \n",
    "3. <font color='red'>Answer to question 3</font> \n",
    "4. <font color='red'>Answer to question 4</font> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
